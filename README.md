# RAG Chatbot Demo: Cloud ‚òÅÔ∏è ‚ÜîÔ∏è Local üíª

A practical demonstration of how to build a **Retrieval Augmented Generation (RAG)** chatbot that can seamlessly switch between cloud-based LLMs (OpenAI) and locally-run LLMs (Ollama). This project showcases the key insight from the presentation: **migrating from cloud to local requires changing only the inference call**.

## üéØ What This Demonstrates

- **Document Processing**: Chunk documents into searchable segments
- **Vector Embeddings**: Convert text to semantic vectors for similarity search
- **RAG Pipeline**: Retrieve relevant context and augment LLM prompts
- **Cloud LLM**: Use OpenAI's GPT models via API
- **Local LLM**: Use Ollama to run models like Llama 3 locally
- **Seamless Switching**: Toggle between cloud and local with a single command

## üìã Prerequisites

### For Cloud LLM (OpenAI)
- OpenAI API key (get one at https://platform.openai.com)

### For Local LLM (Ollama)
- Ollama installed (https://ollama.com)
- At least one model downloaded (e.g., `ollama pull llama3`)

### System Requirements
- Python 3.8+
- 8GB RAM minimum (16GB recommended for larger models)
- Optional: GPU for faster local inference

## üöÄ Quick Start

### 1. Clone and Setup

```bash
cd /home/narasimhan/workarea/local-llm
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Configure Environment

The application supports multiple configuration sources:

**Option A: Use existing Streamlit secrets (Recommended if you use Streamlit)**

If you already have OpenAI credentials in `~/.streamlit/secrets.toml`, you're all set! The application will automatically use them.

Test your configuration:
```bash
python config.py  # Shows which config source is being used
```

**Option B: Use .env file**

```bash
cp config.env.example .env
```

Edit `.env` and set your OpenAI API key (if using cloud LLM):
```
OPENAI_API_KEY=sk-your-actual-key-here
LLM_PROVIDER=openai  # or "ollama" for local
```

See `CONFIGURATION.md` for detailed configuration options.

### 3. Install Ollama (for local LLM)

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Download a model (choose one)
ollama pull llama3        # Recommended: balanced performance
ollama pull mistral       # Alternative: faster, good reasoning
ollama pull phi3          # Alternative: smaller, resource-efficient
```

### 4. Run the Demo

```bash
python chatbot_cli.py
```

## üéÆ Using the Chatbot

### Interactive Commands

Once running, you can use these commands:

- `ask` - Ask a question to the chatbot
- `switch` - Toggle between OpenAI and Ollama
- `info` - Show current configuration
- `examples` - Display example questions
- `help` - Show available commands
- `quit` - Exit the application

### Example Session

```
> ask
  Your question: How do I install Ollama?

==================================================
Question: How do I install Ollama?
Model: llama3 (ollama)
==================================================

To install Ollama, simply run:
```
curl -fsSL https://ollama.com/install.sh | sh
```

Then download a model:
```
ollama pull llama3
```

[Based on Document 1 - ollama_guide.md]
==================================================

> switch
‚úì Switched to OPENAI (gpt-3.5-turbo)
  ‚Üí Now using cloud-based inference

> ask
  Your question: What are the benefits of RAG?
[Answer generated by OpenAI...]
```

## üìö Project Structure

```
local-llm/
‚îú‚îÄ‚îÄ chatbot_cli.py           # Interactive CLI application
‚îú‚îÄ‚îÄ rag_chatbot.py           # Core RAG chatbot logic
‚îú‚îÄ‚îÄ vector_store.py          # Vector database and embeddings
‚îú‚îÄ‚îÄ document_processor.py    # Document chunking and processing
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ config.env.example       # Configuration template
‚îú‚îÄ‚îÄ .env                     # Your configuration (create from example)
‚îú‚îÄ‚îÄ sample_docs/             # Sample documents (auto-generated)
‚îÇ   ‚îú‚îÄ‚îÄ ollama_guide.md
‚îÇ   ‚îú‚îÄ‚îÄ rag_explained.md
‚îÇ   ‚îî‚îÄ‚îÄ local_vs_cloud.md
‚îú‚îÄ‚îÄ chroma_db/               # Vector store persistence (auto-created)
‚îî‚îÄ‚îÄ README.md                # This file
```

## üîß How It Works

### 1. Document Processing

Documents are loaded, split into chunks, and processed:

```python
from document_processor import DocumentChunker

chunker = DocumentChunker(chunk_size=500, chunk_overlap=50)
docs = chunker.load_documents_from_directory("./sample_docs")
chunks = chunker.process_documents(docs)
```

### 2. Vector Storage

Chunks are embedded and stored in ChromaDB:

```python
from vector_store import VectorStore

vector_store = VectorStore(
    embedding_provider="local",  # or "openai"
    persist_directory="./chroma_db"
)
vector_store.add_documents(chunks)
```

### 3. RAG Query Processing

When you ask a question:

1. **Query Embedding**: Your question is converted to a vector
2. **Similarity Search**: Most relevant chunks are retrieved
3. **Context Building**: Retrieved chunks form the context
4. **Prompt Assembly**: Question + context = complete prompt
5. **LLM Inference**: Either OpenAI or Ollama generates the answer

### 4. The Key Insight: Only Inference Changes

```python
# Cloud inference
response = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...]
)

# Local inference
response = ollama.chat(
    model="llama3",
    messages=[...]
)
```

Everything else (chunking, embeddings, retrieval, prompt) stays the same!

## üéì Educational Use

This demo is designed to teach:

1. **RAG Fundamentals**: How retrieval augments generation
2. **Cloud vs Local Trade-offs**: When to use each approach
3. **Practical Migration**: How easy it is to switch inference engines
4. **Vector Search**: Semantic similarity for document retrieval
5. **Production Patterns**: Modular, extensible architecture

## üõ†Ô∏è Advanced Usage

### Using Your Own Documents

Replace the sample documents with your own:

```bash
rm -rf sample_docs/
mkdir sample_docs/
# Add your .txt or .md files to sample_docs/
```

The chatbot will automatically process them on next run.

### Adjusting Chunk Size

Experiment with different chunk sizes in `.env`:

```
CHUNK_SIZE=1000      # Larger chunks = more context per chunk
CHUNK_OVERLAP=100    # More overlap = better continuity
```

### Trying Different Models

For Ollama:
```bash
# Pull additional models
ollama pull mistral
ollama pull phi3
ollama pull codellama

# Update .env
OLLAMA_MODEL=mistral
```

For OpenAI:
```
OPENAI_MODEL=gpt-4      # More capable but more expensive
OPENAI_MODEL=gpt-3.5-turbo  # Faster and cheaper
```

### Using OpenAI Embeddings

For better embedding quality (requires API key):

```python
vector_store = VectorStore(
    embedding_provider="openai",
    openai_model="text-embedding-3-small"
)
```

## üìä Performance Comparison

| Aspect | Cloud (OpenAI) | Local (Ollama) |
|--------|----------------|----------------|
| **Cost** | ~$0.002 per 1K tokens | Free after hardware investment |
| **Speed** | Network latency + processing | CPU/GPU speed dependent |
| **Privacy** | Data sent to OpenAI | Data stays local |
| **Quality** | GPT-4 > local models | Good for most use cases |
| **Offline** | ‚ùå Requires internet | ‚úÖ Works offline |
| **Setup** | API key only | Install + download models |

## üêõ Troubleshooting

### "Ollama not available"

1. Ensure Ollama is installed: `which ollama`
2. Check if Ollama is running: `ollama list`
3. Pull a model: `ollama pull llama3`
4. Test manually: `ollama run llama3`

### "OpenAI API key not set"

1. Create `.env` from `config.env.example`
2. Add your API key: `OPENAI_API_KEY=sk-...`
3. Ensure the file is in the project root

### "No documents found"

The sample documents are created automatically on first run. If missing:

```bash
python document_processor.py
```

### Vector store errors

Clear and rebuild the vector database:

```bash
rm -rf chroma_db/
python chatbot_cli.py  # Will rebuild automatically
```

## üîÆ Next Steps

Enhance this demo by:

1. **Add Web Interface**: Build a Gradio or Streamlit UI
2. **Multi-modal**: Add support for PDF, DOCX, HTML documents
3. **Fine-tuning**: Fine-tune local models on your domain data
4. **Evaluation**: Add metrics to compare cloud vs local quality
5. **Streaming**: Implement streaming responses for better UX
6. **Multiple Models**: Compare multiple models side-by-side
7. **Production Ready**: Add logging, error handling, API endpoints

## üìñ Related Resources

- [Ollama Documentation](https://ollama.com/docs)
- [OpenAI API Reference](https://platform.openai.com/docs)
- [ChromaDB Documentation](https://docs.trychroma.com)
- [RAG Explained](https://www.pinecone.io/learn/retrieval-augmented-generation/)

## üìÑ License

See LICENSE file for details.

## ü§ù Contributing

This is a demonstration project. Feel free to fork and enhance it for your own learning!

---

**Key Takeaway**: Switching from cloud to local LLMs is just a function call away. The entire RAG pipeline remains unchanged ‚Äì you're simply swapping where the text generation happens. üéØ

